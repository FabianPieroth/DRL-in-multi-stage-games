# Main configurations file with default values.

# Available game environments
valid_envs: ["rockpaperscissors", "sequential_auction", "signaling_contest", "simple_soccer"]

# Path setting for logging
log_path: './logs/'
experiment_log_path: ''
experiment_name: ''
delete_logs_after_training: false

device: 1  # Which device to use, e.g., "cpu", "cuda:X"
seed: 0  # Random seed

# Set number of outer and inner iterations 
num_envs: 8_192
iteration_num: 1000
n_steps_per_iteration: null  # TODO @Fabian: Why is this also null?
total_training_steps: null
# total_training_steps = n_steps_per_iteration * num_envs * iteration_num

eval_freq: 400  # How often the evaluation should be run in between learning
verify_br: false  # Wether or not to use the best response verifier

# List of algorithms that maps agents to algorithms. If list of length one is
# used, all agents use same algorithm. 
algorithms: ["ppo"]

# Let agents use the same policy in "symmetric" games. We stick to having a
# list of as many learners as agents just that all indices point to the same
# learner.
policy_sharing: false

# Load default sub-configurations for environment, algorithm, etc.
defaults:
  - rl_envs: sequential_auction
  - algorithm_configs: all_algos
  - space_translators: default_config
  - verifier: default_verifier
  - _self_
